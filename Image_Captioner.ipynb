{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT8IepblmV1U",
        "outputId": "fa57d43e-7f63-408d-eb96-9d78b1189890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq54f3wtASgj",
        "outputId": "9d9a3929-2b57-4c8a-8fea-6024f4f4227f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla K80\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading Datas"
      ],
      "metadata": {
        "id": "Vmao6dyMEnUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load(\"en\")"
      ],
      "metadata": {
        "id": "nniHV9s1EpDq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]"
      ],
      "metadata": {
        "id": "qawtULYSEp6L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        self.df = pd.read_csv(captions_file, error_bad_lines=False, names=['image', 'caption'], header=None, sep='\\t')# , nrows=200)\n",
        "        self.transform = transform\n",
        "        # Get img, caption columns\n",
        "        self.df[\"image\"] = self.df[\"image\"].apply(lambda x: self.removeNumbers(x))\n",
        "        self.imgs = self.df[\"image\"]\n",
        "\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def removeNumbers(self, image):\n",
        "          image = image.split('#')[0]\n",
        "          check = image.split('.')\n",
        "          if len(check)>2:\n",
        "            image = check[0]+\".\"+check[1]\n",
        "          return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)"
      ],
      "metadata": {
        "id": "7qqOI9CQEsKZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "        return imgs, targets"
      ],
      "metadata": {
        "id": "nqshKCqSEuKm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(\n",
        "    root_folder,annotation_file,transform,batch_size=32,\n",
        "    num_workers=8,shuffle=True,pin_memory=True,train_test_ratio=0.0):\n",
        "  \n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "    dataset_train, dataset_test = torch.utils.data.random_split(dataset, \n",
        "                                  [int(len(dataset)*train_test_ratio), len(dataset)-int(len(dataset)*train_test_ratio)],\n",
        "                                  generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=dataset_train,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=dataset_test,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return train_loader, dataset_train, test_loader, dataset_test, dataset"
      ],
      "metadata": {
        "id": "p01d9nK2Exi-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### implementing Networks"
      ],
      "metadata": {
        "id": "zkDBD2XDVNCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, hidden_dim, output_dim = 1):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.W_a = nn.Linear(self.num_features, self.hidden_dim)\n",
        "        self.U_a = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.v_a = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "                \n",
        "    def forward(self, features, decoder_hidden):\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
        "        atten_1 = self.W_a(features)\n",
        "        atten_2 = self.U_a(decoder_hidden)\n",
        "        atten_tan = torch.tanh(atten_1+atten_2)\n",
        "        atten_score = self.v_a(atten_tan)\n",
        "        atten_weight = F.softmax(atten_score, dim = 1)\n",
        "        context = torch.sum(atten_weight * features,  dim = 1)\n",
        "        atten_weight = atten_weight.squeeze(dim=2)\n",
        "        \n",
        "        return context, atten_weight\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        batch, feature_maps, size_1, size_2 = features.size()       \n",
        "        features = features.permute(0, 2, 3, 1)\n",
        "        features = features.view(batch, size_1*size_2, feature_maps)\n",
        "        \n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, num_features):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.embedding_dim = embed_size\n",
        "        self.hidden_dim = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sample_temp = 0.5\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTMCell(embed_size + num_features, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.attention = BahdanauAttention(num_features, hidden_size)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.init_h = nn.Linear(num_features, hidden_size)\n",
        "        self.init_c = nn.Linear(num_features, hidden_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, features, captions, sample_prob = 0.0):\n",
        "        embed = self.embeddings(captions)\n",
        "        h, c = self.init_hidden(features)\n",
        "        seq_len = embed.size(1)\n",
        "        feature_size = features.size(1)\n",
        "        batch_size = embed.size(0)\n",
        "        \n",
        "        outputs = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n",
        "        atten_weights = torch.zeros(batch_size, seq_len, feature_size).to(device)\n",
        "        word_embed = embed[:,0,:]\n",
        "        for t in range(seq_len):\n",
        "\n",
        "            context, atten_weight = self.attention(features, h)\n",
        "            # input_concat shape at time step t = (batch, embedding_dim + hidden_dim)\n",
        "            input_concat = torch.cat([word_embed, context], 1)\n",
        "            h, c = self.lstm(input_concat, (h,c))\n",
        "            h = self.drop(h)\n",
        "            output = self.fc(h)\n",
        "            outputs[:, t, :] = output\n",
        "            atten_weights[:, t, :] = atten_weight\n",
        "\n",
        "            scoring = F.log_softmax(output, dim=1)\n",
        "            top_idx = scoring.topk(1)[1]\n",
        "            word_embed = self.embeddings(top_idx).squeeze(1)\n",
        "            # print(top_idx.shape)\n",
        "            \n",
        "        return outputs, atten_weights\n",
        "\n",
        "\n",
        "    def init_hidden(self, features):\n",
        "        mean_annotations = torch.mean(features, dim = 1)\n",
        "        h0 = self.init_h(mean_annotations)\n",
        "        c0 = self.init_c(mean_annotations)\n",
        "        return h0, c0\n",
        "    def greedy_search(self, features, vocabulary, max_sentence = 40):\n",
        "\n",
        "        sentence = []\n",
        "        weights = []\n",
        "        input_word = torch.tensor([vocabulary.stoi['<SOS>']]).to(device)\n",
        "        h, c = self.init_hidden(features)\n",
        "        while True:\n",
        "            embedded_word = self.embeddings(input_word)\n",
        "            context, atten_weight = self.attention(features, h)\n",
        "            # input_concat shape at time step t = (batch, embedding_dim + context size)\n",
        "            input_concat = torch.cat([embedded_word, context],  dim = 1)\n",
        "            h, c = self.lstm(input_concat, (h,c))\n",
        "            h = self.drop(h)\n",
        "            output = self.fc(h)\n",
        "            scoring = F.log_softmax(output, dim=1)\n",
        "            top_idx = scoring[0].topk(1)[1]\n",
        "            sentence.append(top_idx.item())\n",
        "            weights.append(atten_weight)\n",
        "            input_word = torch.tensor(top_idx)\n",
        "            if (len(sentence) >= max_sentence or top_idx == vocabulary.stoi['<EOS>']):\n",
        "                break\n",
        "        return sentence, weights\n",
        "\n",
        "\n",
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, num_features):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, num_features)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs, _ = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=40):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image)\n",
        "            sentence, weights = self.decoderRNN.greedy_search(x, vocabulary, max_sentence = max_length)\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in sentence]\n",
        "\n",
        "\n",
        "    def init_hidden(self, features):\n",
        "        mean_annotations = torch.mean(features, dim = 1)\n",
        "        h0 = self.init_h(mean_annotations)\n",
        "        c0 = self.init_c(mean_annotations)\n",
        "        return h0, c0\n",
        "\n"
      ],
      "metadata": {
        "id": "iMu5cgXIRYpr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "xfRQCz3_cljT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_dir = \"/content/drive/My Drive/Courses/DeepLearning/HW03/Q03/my_checkpoint.pth.tar\"\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=model_save_dir):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "load_model = True\n",
        "save_model = True\n",
        "train_CNN = False\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_features = 2048\n",
        "num_layers = 1\n",
        "learning_rate = 1e-4\n",
        "momentum = 0.9\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([ \n",
        "    transforms.Resize(256),                          \n",
        "    transforms.RandomCrop(224),                     \n",
        "    transforms.RandomHorizontalFlip(),               \n",
        "    transforms.ToTensor(),                           \n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "train_loader, dataset_train, test_loader, dataset_test, dataset = get_loader(\n",
        "    root_folder=\"/content/drive/My Drive/Courses/DeepLearning/HW03/Q03/Datas/Flicker8k_Dataset\",\n",
        "    annotation_file=\"/content/drive/My Drive/Courses/DeepLearning/HW03/Q03/Datas/Flickr8k.token.txt\",\n",
        "    transform=transform,\n",
        "    num_workers=2,\n",
        "    batch_size=batch_size,\n",
        "    train_test_ratio=0.95\n",
        ")\n",
        "vocab_size = len(dataset.vocab)\n",
        "\n",
        "\n",
        "# initialize model, loss etc\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers, num_features).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "Nd8YVcTparg0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_log(file_path, stat):\n",
        "  with open(file_path, 'a') as the_file:\n",
        "    the_file.write(stat+\"\\n\")"
      ],
      "metadata": {
        "id": "SBl922587Vu3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_examples(imgs):\n",
        "\n",
        "  print(\"\\n\")\n",
        "  img2plot = imgs[0].cpu().permute(1,2,0)-torch.min(imgs[0].cpu().permute(1,2,0))\n",
        "  img2plot = img2plot/torch.max(img2plot)\n",
        "  plt.imshow(img2plot)\n",
        "  plt.show()\n",
        "  with torch.no_grad():\n",
        "    predicted_caption = model.caption_image(imgs[0].unsqueeze(0), dataset.vocab)\n",
        "  print(\"\\n\"+\" \".join(predicted_caption))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "Pq-penTF9bYr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation():\n",
        "  \n",
        "  Loss = 0\n",
        "  Acc = 0\n",
        "  with torch.no_grad():\n",
        "    for idx, (imgs, captions) in enumerate(test_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        captions = captions.to(device)\n",
        "        captions = captions.permute(1,0)\n",
        "        \n",
        "        outputs = model(imgs, captions)\n",
        "\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions.reshape(-1))\n",
        "        acc = torch.sum(outputs.topk(1)[1].reshape(-1) == captions.reshape(-1))/ captions.reshape(-1).shape[0]\n",
        "        Acc = Acc+acc\n",
        "        Loss = Loss+loss.item()\n",
        "\n",
        "  Acc = Acc/(idx+1)\n",
        "  Loss = Loss/(idx+1)\n",
        "  return Loss, Acc"
      ],
      "metadata": {
        "id": "tBdDz9IazYqb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(print_every):\n",
        "\n",
        "    if load_model:\n",
        "      load_checkpoint(torch.load(model_save_dir), model, optimizer)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        step = 0\n",
        "\n",
        "        if save_model:\n",
        "          checkpoint = {\n",
        "              \"state_dict\": model.state_dict(),\n",
        "              \"optimizer\": optimizer.state_dict(),\n",
        "              \"step\": step,}\n",
        "          save_checkpoint(checkpoint)\n",
        "        num_itters = len(train_loader)\n",
        "        for idx, (imgs, captions) in enumerate(train_loader):\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "            captions = captions.permute(1,0)\n",
        "            \n",
        "            outputs = model(imgs, captions)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, vocab_size), captions.reshape(-1))\n",
        "            acc = torch.sum(outputs.topk(1)[1].reshape(-1) == captions.reshape(-1))/ captions.reshape(-1).shape[0]\n",
        "            \n",
        "            if step % print_every == 0:\n",
        "              loss_val, acc_val = validation()\n",
        "              stats = '{2}/{3} - {0}/{1} |  Loss train-val = {4:.4f} - {6:.4f} | Acc train - val = {5:.4f} - {7:.4f}'.format(\n",
        "                    idx, num_itters, epoch, num_epochs, loss.item(), acc, loss_val, acc_val)\n",
        "              print('\\r' + stats)\n",
        "              write_log(\"/content/drive/My Drive/Courses/DeepLearning/HW03/Q03/Logs.txt\", stats)\n",
        "\n",
        "            step += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(loss)\n",
        "            optimizer.step()\n",
        "            \n",
        "        plot_examples(imgs)\n"
      ],
      "metadata": {
        "id": "0ddK9GL9VksV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOvz7BK4dbpf",
        "outputId": "8c7ff77d-1411-4c12-b0ba-064bee0ca4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n",
            "=> Saving checkpoint\n",
            "0/100 - 0/601 |  Loss train-val = 3.4668 - 3.7037 | Acc train - val = 0.1697 - 0.1396\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Vmao6dyMEnUl",
        "zkDBD2XDVNCp"
      ],
      "name": "Image_Captioner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}